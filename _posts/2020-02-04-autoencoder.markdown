---
layout: post
title: Autoencoder
date: 2020-02-04 9:12:20 -0500
description: Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: post/demo_autoencoder.png # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Autoencoder]
---

The autoencoder is a neural networks that is trained to try to copy its input to its output. It is composed of two parts: an <b>encoder</b> that is trained to generate a code $$\boldsymbol{h}$$ that describe the input $$\boldsymbol{x}$$. The encoder can be represented by the function $$\boldsymbol{f}$$ and the code generated by the encoder is:

$$
h = f(x)
$$

The second part is a <b>decoder</b> , it produces a reconstruction from the code $$\boldsymbol{h}$$ generated by encoder. It can be represented by the function $$\boldsymbol{g}$$ and its ouput is:

$$
x' = g(h) \textrm{ or }  
x' = g(f(x))
$$

The architecture is shown in next Figure:


![Autoencoder]({{site.baseurl}}/assets/img/post/autoencoder.png)

<b>Training process</b>

In the process of training an autoencoder, it is prioritized that the model learns to which aspect of the input should be copied. In other words, the autoencoder tries to learn an approximation to the identity function where the output $$\boldsymbol{x'}$$ is similar to the input $$\boldsymbol{x}$$. 

The autoencoder is an interesting network because it has a hidden layer that the output is the code $$\boldsymbol{h}$$. If the size of $$\boldsymbol{h}$$ code is less than the size of the input $$\boldsymbol{x}$$, the model is forced to learn the useful properties of the data. Then, $$\boldsymbol{h}$$ contains the appropriate information to the decoder reproduces an output $$\boldsymbol{x'}$$ similar to the input $$\boldsymbol{x}$$. Therefore, the autoencoder can compress the input data in a $$\boldsymbol{h}$$ code that is a latent-space representation.

<!-- <b>Loss function</b> -->

The autoencoder is tipically  used for:
<ul>
  <li>Dimensionality reduction</li>
  <li>Denoising</li>
  <li>Anomaly detection</li>
</ul>

<b>Code in PyTorch</b>

<ul>
  <li><a>Dimensionality reduction</a></li>
  <li><a href="https://github.com/LizLima/Dive-into-DL/blob/master/Autoencoder/dae.py">Denoising</a></li>
  <li><a>Anomaly detection</a></li>
</ul>